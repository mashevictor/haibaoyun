#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ†æPlaywrightæµ‹è¯•ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
"""
import os
import json
import re
from pathlib import Path

def analyze_test_results():
    """åˆ†ææµ‹è¯•ç»“æœ"""
    print("=" * 60)
    print("PowerVerse Chain æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥æµ‹è¯•ç»“æœç›®å½•
    test_results_dir = Path("test-results")
    playwright_report_dir = Path("playwright-report")
    
    issues = {
        'chinese_found': [],
        'empty_links': [],
        'other_issues': []
    }
    
    # æ£€æŸ¥æˆªå›¾æ–‡ä»¶ï¼ˆè¡¨ç¤ºå‘ç°äº†ä¸­æ–‡ï¼‰
    if test_results_dir.exists():
        screenshot_files = list(test_results_dir.glob("*_chinese_found.png"))
        for screenshot in screenshot_files:
            page_name = screenshot.stem.replace("_chinese_found", "")
            issues['chinese_found'].append({
                'page': page_name,
                'screenshot': str(screenshot)
            })
    
    # åˆ†ææ‰€æœ‰é¡µé¢
    pages_to_check = [
        'index.html',
        'pages/scenarios.html',
        'pages/token.html',
        'pages/hardware-factory.html',
        'pages/research-strength.html',
        'pages/developer.html',
        'pages/about.html',
        'pages/chain.html',
        'pages/infra.html',
        'pages/market.html',
        'pages/dao.html',
        'pages/decloud.html',
    ]
    
    print("ğŸ“‹ å‘ç°çš„é—®é¢˜:")
    print()
    
    if issues['chinese_found']:
        print("âŒ è‹±æ–‡ç¯å¢ƒä¸‹å‘ç°ä¸­æ–‡çš„é¡µé¢:")
        for issue in issues['chinese_found']:
            print(f"   â€¢ {issue['page']}")
        print()
    
    # æ£€æŸ¥HTMLæ–‡ä»¶ä¸­çš„ç©ºé“¾æ¥
    print("ğŸ” æ£€æŸ¥ç©ºé“¾æ¥...")
    print()
    
    empty_links_found = []
    for page_path in pages_to_check:
        if os.path.exists(page_path):
            with open(page_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æŸ¥æ‰¾ç©ºé“¾æ¥
            empty_link_pattern = r'<a[^>]*href=["\']#["\'][^>]*>([^<]*)</a>'
            matches = re.findall(empty_link_pattern, content)
            
            if matches:
                empty_links_found.append({
                    'page': page_path,
                    'count': len(matches),
                    'links': matches[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
                })
    
    if empty_links_found:
        print("âš ï¸  å‘ç°ç©ºé“¾æ¥çš„é¡µé¢:")
        for item in empty_links_found:
            print(f"   â€¢ {item['page']}: {item['count']} ä¸ªç©ºé“¾æ¥")
            for link_text in item['links']:
                if link_text.strip():
                    print(f"     - \"{link_text.strip()[:50]}\"")
        print()
    else:
        print("âœ… æœªå‘ç°ç©ºé“¾æ¥")
        print()
    
    # ç”Ÿæˆä¿®å¤å»ºè®®
    print("=" * 60)
    print("ğŸ’¡ ä¿®å¤å»ºè®®:")
    print("=" * 60)
    print()
    
    if issues['chinese_found']:
        print("1. è‹±æ–‡ç¯å¢ƒä¸‹å‘ç°ä¸­æ–‡çš„é¡µé¢éœ€è¦:")
        print("   â€¢ æ£€æŸ¥è¿™äº›é¡µé¢çš„HTMLæ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰ä¸­æ–‡æ–‡æœ¬éƒ½æœ‰data-i18nå±æ€§")
        print("   â€¢ æ£€æŸ¥js/languages.jsï¼Œç¡®ä¿å¯¹åº”çš„ç¿»è¯‘é”®å­˜åœ¨ä¸”æ­£ç¡®")
        print("   â€¢ éªŒè¯getTranslationå‡½æ•°æ˜¯å¦æ­£ç¡®å¤„ç†åµŒå¥—é”®")
        print()
    
    if empty_links_found:
        print("2. ç©ºé“¾æ¥éœ€è¦:")
        print("   â€¢ ä¸ºè¿™äº›é“¾æ¥æ·»åŠ å®é™…çš„hrefå±æ€§")
        print("   â€¢ æˆ–è€…æ·»åŠ title='Coming soon'å±æ€§è¯´æ˜")
        print("   â€¢ æˆ–è€…ç§»é™¤è¿™äº›é“¾æ¥å¦‚æœä¸éœ€è¦")
        print()
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    report = {
        'timestamp': str(Path.cwd()),
        'issues': issues,
        'empty_links': empty_links_found,
        'recommendations': []
    }
    
    with open('test_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: test_analysis_report.json")
    print()

if __name__ == '__main__':
    analyze_test_results()
